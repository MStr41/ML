# -*- coding: utf-8 -*-
"""ItemKNN(Updated_nDCG)_LensKit_Amazon_toys_and_Games_UserBasedSplit+10_Core_Pruning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15kLxWIv7JjyhiM4D6IahIx4RFyK9dv8k
"""

from lenskit.algorithms import Recommender, item_knn as knn
from lenskit import batch, topn, util
import pandas as pd
import joblib
import gzip
import json
from lenskit import crossfold as xf
import seedbank

"""
Previous Behavior (Before Update):
Before the modification, the nDCG_LK class in LensKit used the following approach:

IDCG Calculation: It calculated the IDCG for the full list length n (i.e., IDCG@n), regardless of the number of items actually rated by the user.
Handling Fewer Rated Items: If fewer items than n were rated, it used the IDCG value computed for n (e.g., IDCG@10 for n=10) even if the user had only rated 5 items.
Example:

Suppose n = 10 and a user has only rated 5 items. The previous implementation would calculate IDCG@10, regardless of the fact that only 5 items were rated.
Updated Behavior (After Modification):
The updated nDCG_LK class now mimics RecPackâ€™s behavior:

IDCG Calculation: It precomputes IDCG values up to n and uses these values based on the actual number of items rated (hist_len). If fewer items are rated than n, it directly uses the precomputed IDCG value for the number of rated items.
Handling Fewer Rated Items: If fewer items than n are rated, it uses the precomputed IDCG value for the exact number of rated items rather than for n.
Example:

Suppose n = 10 and a user has only rated 5 items. The updated implementation would use IDCG@5, not IDCG@10, because it directly refers to the precomputed IDCG value for 5 rated items.

Conclusion:
Previous Behavior: Always used IDCG@n, regardless of the actual number of rated items.
Updated Behavior: Uses IDCG@hist_len, where hist_len is the number of items actually rated by the user.
"""
import numpy as np

class nDCG_LK:
    def __init__(self, n, top_items, test_items):
        self.n = n
        self.top_items = top_items
        self.test_items = test_items

    def _ideal_dcg(self):
        iranks = np.zeros(self.n, dtype=np.float64)
        iranks[:] = np.arange(1, self.n + 1)
        idcg = np.cumsum(1.0 / np.log2(iranks + 1), axis=0)
        if len(self.test_items) < self.n:
            idcg[len(self.test_items):] = idcg[len(self.test_items) - 1]
        return idcg[self.n - 1]

    def calculate_dcg(self):
        dcg = 0
        for i, item in enumerate(self.top_items):
            if item in self.test_items:
                relevance = 1
            else:
                relevance = 0
            rank = i + 1
            contribution = relevance / np.log2(rank + 1)
            dcg += contribution
        return dcg

    def calculate(self):
        dcg = self.calculate_dcg()
        ideal_dcg = self._ideal_dcg()
        if ideal_dcg == 0:
            return 0
        ndcg = dcg / ideal_dcg
        return ndcg

# Function to load the JSON data
def load_json_data(file_path, chunksize=10000):
    chunks = pd.read_json(file_path, lines=True, compression='gzip', chunksize=chunksize)
    return pd.concat(chunks, ignore_index=True)
# Load and preprocess ratings data
file_path = 'Video_Games_5.json.gz'
ratings = load_json_data(file_path)
print(len(ratings))
ratings = ratings.rename(columns={'reviewerID': 'user', 'asin': 'item', 'overall': 'rating'})
ratings = ratings.dropna(subset=['rating'])
# Convert 'rating' column to float
ratings['rating'] = ratings['rating'].astype(float)
# Keep only the necessary columns
ratings = ratings[['user', 'item', 'rating']]
print(ratings.head())
print(len(ratings))

# Convert user and item IDs to integers
ratings['user'], user_index = pd.factorize(ratings['user'])
ratings['item'], item_index = pd.factorize(ratings['item'])
print(ratings.head())
print(len(ratings))

# Inspect the ratings data
print("Initial Ratings Data Inspection:")
print("Number of interactions:", len(ratings))
print("Number of unique users:", ratings['user'].nunique())
print("Number of unique items:", ratings['item'].nunique())

# Check for users and items with fewer than 10 interactions
user_counts = ratings['user'].value_counts()
item_counts = ratings['item'].value_counts()

print("\nUsers with fewer than 10 interactions:", (user_counts < 10).sum())
print("Items with fewer than 10 interactions:", (item_counts < 10).sum())

# Check for empty rows
empty_rows = ratings.isnull().sum().sum()
print("\nNumber of empty rows:", empty_rows)

# Check for duplicate rows
duplicate_rows = ratings.duplicated().sum()
print("Number of duplicate rows:", duplicate_rows)
# Check for duplicate ratings (same user, same item)
duplicate_ratings = ratings.duplicated(subset=['user', 'item']).sum()
print("Number of duplicate ratings (same user, same item):", duplicate_ratings)

# Remove duplicate rows
ratings = ratings.drop_duplicates()
# Aggregate duplicate ratings (same user, same item) by averaging their ratings
ratings = ratings.groupby(['user', 'item'], as_index=False)['rating'].mean()

# Check for empty rows after cleaning
empty_rows = ratings.isnull().sum().sum()
print("\nNumber of empty rows after cleaning:", empty_rows)

# Check for duplicate rows after cleaning
duplicate_rows = ratings.duplicated().sum()
print("Number of duplicate rows after cleaning:", duplicate_rows)

# Check for duplicate ratings (same user, same item) after cleaning
duplicate_ratings = ratings.duplicated(subset=['user', 'item']).sum()
print("Number of duplicate ratings (same user, same item) after cleaning:", duplicate_ratings)

print(len(ratings))

# 10-core pruning
def prune_10_core(data):
    while True:
        # Filter users with fewer than 10 interactions
        user_counts = data['user'].value_counts()
        valid_users = user_counts[user_counts >= 10].index
        data = data[data['user'].isin(valid_users)]

        # Filter items with fewer than 10 interactions
        item_counts = data['item'].value_counts()
        valid_items = item_counts[item_counts >= 10].index
        data = data[data['item'].isin(valid_items)]

        # Check if no more pruning is needed
        if all(user_counts >= 10) and all(item_counts >= 10):
            break
    return data

# Apply 10-core pruning
ratings = prune_10_core(ratings)

# Inspect the pruned ratings data
print("\nAfter Pruning:")
print("Number of interactions:", len(ratings))
print("Number of unique users:", ratings['user'].nunique())
print("Number of unique items:", ratings['item'].nunique())

# Check for users and items with fewer than 10 interactions after pruning
user_counts = ratings['user'].value_counts()
item_counts = ratings['item'].value_counts()

print("\nUsers with fewer than 10 interactions after pruning:", (user_counts < 10).sum())
print("Items with fewer than 10 interactions after pruning:", (item_counts < 10).sum())

# Split into train and test sets
final_test_method = xf.SampleFrac(0.10, rng_spec=42)

train_parts = []
test_parts = []

for tp in xf.partition_users(ratings, 1, final_test_method):
    train_parts.append(tp.train)
    test_parts.append(tp.test)

train_data = pd.concat(train_parts)
final_test_data = pd.concat(test_parts)

# Check and print the number of interactions and users in each set

print("Train Data - Number of Interactions:", len(train_data))

print("Final Test Data - Number of Interactions:", len(final_test_data))

print("Train Data - Number of Users:", train_data['user'].nunique())

print("Final Test Data - Number of Users:", final_test_data['user'].nunique())

# Split train data into train and validation sets
validation_split_method = xf.SampleFrac(0.1111, rng_spec=42)

train_parts = []
valid_parts = []

for tp in xf.partition_users(train_data, 1, validation_split_method):
    train_parts.append(tp.train)
    valid_parts.append(tp.test)

pure_train_data = pd.concat(train_parts)
validation_data = pd.concat(valid_parts)

# Check and print the number of interactions and users in each set
print("\nBefore Splitting:")
print("Pure Train Data - Number of Interactions:", len(pure_train_data))
print("Validation Data - Number of Interactions:", len(validation_data))
print("Final Test Data - Number of Interactions:", len(final_test_data))

print("Pure Train Data - Number of Users:", pure_train_data['user'].nunique())
print("Validation Data - Number of Users:", validation_data['user'].nunique())
print("Final Test Data - Number of Users:", final_test_data['user'].nunique())

# Downsample the training set to different% of interactions for each user using xf.SampleFrac
##########################################################################
import sys
try:
    fraction_value = float(sys.argv[1])  
except (IndexError, ValueError):
    fraction_value = 0.7
downsample_fraction = fraction_value
##########################################################################
downsample_method = xf.SampleFrac(1.0 - downsample_fraction, rng_spec=42)
downsampled_train_parts = []

for i, tp in enumerate(xf.partition_users(pure_train_data, 1, downsample_method)):
    downsampled_train_parts.append(tp.train)

# Combine downsampled train parts into one DataFrame
downsampled_train_data = pd.concat(downsampled_train_parts)

# Checks for number of interactions and users in each set after downsampling
print("\nAfter Downsampling:")
print("Downsampled Train Data - Number of Interactions:", len(downsampled_train_data))
print("Validation Data - Number of Interactions:", len(validation_data))
print("Final Test Data - Number of Interactions:", len(final_test_data))

print("Downsampled Train Data - Number of Users:", downsampled_train_data['user'].nunique())
print("Validation Data - Number of Users:", validation_data['user'].nunique())
print("Final Test Data - Number of Users:", final_test_data['user'].nunique())

def evaluate_with_ndcg(aname, algo, train, valid):
    fittable = util.clone(algo)
    fittable = Recommender.adapt(fittable)
    fittable.fit(train)
    users = valid.user.unique()
    recs = batch.recommend(fittable, users, 10,n_jobs = 1)
    recs['Algorithm'] = aname

    total_ndcg = 0
    for user in users:
        user_recs = recs[recs['user'] == user]['item'].values
        user_truth = valid[valid['user'] == user]['item'].values
        ndcg_score = nDCG_LK(10, user_recs, user_truth).calculate()
        total_ndcg += ndcg_score

    mean_ndcg = total_ndcg / len(users)
    return recs, mean_ndcg

results = []
best_k = None
best_mean_ndcg = -float('inf')

k_values = [1, 3, 5, 7, 20]

for k in k_values:
    seedbank.initialize(42)
    algo_ii = knn.ItemItem(nnbrs=k, center=False, aggregate='sum', feedback="explicit")

    valid_recs, mean_ndcg = evaluate_with_ndcg('ItemItem', algo_ii, downsampled_train_data, validation_data)
    results.append({'K': k, 'Mean nDCG': mean_ndcg})

    if mean_ndcg > best_mean_ndcg:
        best_mean_ndcg = mean_ndcg
        best_k = k

print("Results:")
for result in results:
    print(f"K = {result['K']}: Mean nDCG = {result['Mean nDCG']:.4f}")

print(f"\nBest K: {best_k} (Mean nDCG = {best_mean_ndcg:.4f})")

# Fit the algorithm on the full training data with the best K
final_algo = knn.ItemItem(nnbrs=best_k, center=False, aggregate='sum', feedback="explicit")

# Use evaluate_with_ndcg to get recommendations and mean nDCG
final_recs, mean_ndcg = evaluate_with_ndcg('ItemItem', final_algo, downsampled_train_data, final_test_data)

print(f"NDCG mean for test set: {mean_ndcg:.4f}")

#################################################
ndcg_value = mean_ndcg
key_name = "itemknn_lenskit_amazon_video_games"

from filelock import FileLock
import os
import json


output_file = "metric_results.json"
lock_file = output_file + ".lock"
fraction_key = str(downsample_fraction)

#Mit lock wird es gesichert
with FileLock(lock_file):
    # Datei lesen und schreiben
    if os.path.exists(output_file):
        with open(output_file, "r") as f:
            try:
                content = json.load(f)
                if not isinstance(content, dict):
                    content = {}
            except json.JSONDecodeError:
                content = {}
    else:
        content = {}

    if key_name not in content:
        content[key_name] = {}

    content[key_name][fraction_key] = ndcg_value

    with open(output_file, "w") as f:
        json.dump(content, f, indent=4)
#################################################
